{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make better use of our iSAID graph for planning, assessment, and other analyses, we need to reduce the very messy data we have from all our heterogeneous sources down to something digestible and meaningful. We also need to connect the various entities that we have as assets (people, datasets, publications, etc.) to concepts that can be linked to broader sources of meaning and other related concepts so that we can broaden the understanding of what any one entity represents.\n",
    "\n",
    "A lot of the questions we want to ask of these data come down to a finite set of concepts that cover the subject matters we deal with as an institution. We have many different listings of terms from our various catalogs that describe these subject matters, but they are all extremely noisy, not at all uniform, and in almost no cases do they reference some type of controlled vocabulary source. Even when they do reference such a source, we are finding that the terms supplied in metadata cannot actually be traced back to the asserted vocabulary source. We also have many unstructured texts of various kinds from simple abstracts to much more complex and lengthy structures that can be tapped but require handling through some type natural language processing to extract useful structure and concepts that can be traced to a source of definition and linkage to other concepts.\n",
    "\n",
    "To approach this problem, we are using named entity recognition, built first with a rule-based approach to establish a training dataset followed by a machine learning process on both unstructured descriptive text and even the structured texts that cannot readily be identified as to source and definition. We can then tie labels we develop through NER processing back to their original source, examine new discovered values for those labels for any applicability to our needs, and feed confirmed subject matters with solid references back into our graph for use.\n",
    "\n",
    "This notebook deals with the process of building our usable reference library. There is no single point of truth or an overarching ontology that is going to meet all our needs. There are instead several different sources that we need to curate into our process, sticking with our overall goal of using software codes to do all the work so we can set up a continuously operating and improving process. The concepts developed through this notebook become entities within the graph when they are firmly linked to some other entity through rule-based or confirmed machine learning named entity recognition. An overarching goal for these concepts is to make sure that everything has at least some type of url that can be applied to reference to where further information about the source can be found. Not all of the concept references that we need to use in describing our work exist yet in robust forms such as ontologies that yield full semantic functionality, so not all of the linkages assembled are actionable in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isaid_helpers\n",
    "import click\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import copy\n",
    "from collections import Counter\n",
    "from pylinkedcmd import wikidata\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USGS Thesaurus Sources\n",
    "The USGS Thesaurus is one of our primary sources for a controlled vocabulary reference point. It has been designed specifically to address USGS subject matters and is in broad use across many of our information systems as the go to point of consultation in picking keywords to describe things like datasets and projects. The USGS Thesaurus is built to the SKOS specification, meaning that it has an inherent hierarchy and concepts of preferred and non-preferred terms. Much of the Thesaurus contains scope notes that provide usable descriptions to help understand meaning and intent of the terms. There is a reasonable API-style interface to much of the USGS Thesaurus, a way to construct resolvable identifiers, and a structured download via a SQLite database that we take advantage of here. The USGS Thesaurus is actually a family of different controlled vocabularies, including a large set of place names that we are pulling all together to use in various ways throughout our knowledge graphing exercise.\n",
    "\n",
    "The major functionality in the following processing codeblocks will be formalized at some point into the pyLinkedCMD package once we settle on the form and structure of the vocabulary reference material. The first codeblock establishes a connection to the thesaurus database with an optional download from original source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you really really need to download the USGS Thesaurus DB from source? [Y/n]: n\n",
      "CPU times: user 120 ms, sys: 37.9 ms, total: 158 ms\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if click.confirm('Do you really really need to download the USGS Thesaurus DB from source?', default=True):\n",
    "    r = requests.get(\"https://apps.usgs.gov/thesaurus/download/thesauri.zip\", stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(isaid_helpers.f_usgs_thesaurus_source, 'wb') as f:\n",
    "            for chunk in r:\n",
    "                f.write(chunk)\n",
    "    if os.path.exists(isaid_helpers.f_usgs_thesaurus_source):\n",
    "        with zipfile.ZipFile(isaid_helpers.f_usgs_thesaurus_source, 'r') as f:\n",
    "            f.extractall(isaid_helpers.local_cache_path_rel)\n",
    "\n",
    "con_thesaurus = sqlite3.connect(f\"{isaid_helpers.local_cache_path_rel}thesauri.db\")\n",
    "df_thesaurus = pd.read_sql_query(\"SELECT * from thesaurus\", con_thesaurus)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following codeblock, we pull each individual thesaurus reference from an index table and then read each one into memory for further processing. We handle the particular dynamic in the USGS Thesaurus itself of leveraging the hierarchy to categories, setting each top-level category up as its own distinct set of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgs_thesaurus_terms = pd.read_sql_query(\"SELECT * FROM term\", con_thesaurus)\n",
    "df_usgs_thesaurus_terms = df_usgs_thesaurus_terms.where(pd.notnull(df_usgs_thesaurus_terms), None)\n",
    "d_terms = df_usgs_thesaurus_terms.to_dict(orient=\"records\")\n",
    "\n",
    "def category_parent(code):\n",
    "    code_item = next((i for i in d_terms if i[\"code\"] == code), None)\n",
    "    if code_item is None:\n",
    "        return\n",
    "\n",
    "    if code_item[\"parent\"] is None:\n",
    "        return\n",
    "    \n",
    "    if code_item[\"parent\"] == 1:\n",
    "        return\n",
    "    \n",
    "    next_parent_code = code_item[\"parent\"]\n",
    "    while True:\n",
    "        parent_item = next((i for i in d_terms if i[\"code\"] == next_parent_code), None)\n",
    "        if parent_item[\"parent\"] == 1:\n",
    "            break\n",
    "        else:\n",
    "            next_parent_code = parent_item[\"parent\"]\n",
    "\n",
    "    return parent_item[\"name\"]\n",
    "\n",
    "all_terms = list()\n",
    "\n",
    "for index, row in df_thesaurus.iterrows():\n",
    "    if row[\"name\"] != \"USGS Thesaurus\":\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {row.tblname}\", con_thesaurus)\n",
    "            df[\"thesaurus_name\"] = row[\"name\"]\n",
    "            df[\"thesaurus_id\"] = row[\"tag\"]\n",
    "            df[\"category\"] = None\n",
    "            d = df.to_dict(orient=\"records\")\n",
    "            all_terms.extend(d)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        for term in d_terms:\n",
    "            if term[\"parent\"] is not None:\n",
    "                term[\"thesaurus_name\"] = \"USGS Thesaurus\"\n",
    "                term[\"thesaurus_id\"] = 2\n",
    "                term[\"parent\"] = int(term[\"parent\"])\n",
    "                term[\"category\"] = category_parent(term[\"code\"])\n",
    "            if \"category\" in term and term[\"category\"] is not None:\n",
    "                all_terms.append(term)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, we prepare the final structure of terms across all thesauri of the USGS Thesaurus \"family,\" adding in a little bit of specific classification for use in NER processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_url(thesaurus_id, code):\n",
    "    return f\"https://apps.usgs.gov/thesaurus/term-simple.php?code={code}&thcode={thesaurus_id}\"\n",
    "\n",
    "def concept_mapping(term):\n",
    "    category_concepts = {'topics': \"USGS_SCIENCE_TOPICS\",\n",
    "         'methods': \"USGS_SCIENTIFIC_METHODS\",\n",
    "         'sciences': \"USGS_SCIENTIFIC_DISCIPLINES\",\n",
    "         'product types': \"USGS_PRODUCT_TYPES\",\n",
    "         'time periods': \"USGS_GEOLOGIC_TIME_PERIODS\",\n",
    "         'institutional structures and activities': \"USGS_INSTITUTIONAL_STRUCTURES_ACTIVITIES\",\n",
    "         'USGS business categories': \"USGS_BUSINESS_CATEGORIES\"}\n",
    "    thesaurus_concepts = {'Common geographic areas': \"USGS_COMMON_GEOGRAPHIC_AREAS\",\n",
    "         'Alexandria Digital Library Feature Type Thesaurus': \"ALEXANDRIA_DIGITAL_LIBRARY_FEATURE_TYPES\",\n",
    "         'Lithologic classification of geologic map units': \"USGS_LITHOLOGY\",\n",
    "         'ISO 19115 Topic Category': \"ISO_19115_TOPICS))\",\n",
    "         'Marine Realms Information Bank (MRIB) keywords': \"MARINE_REALMS_INFORMATION_BANK\",\n",
    "         'Coastal and Marine Ecological Classification Standard': \"CMECS\"}\n",
    "    \n",
    "    if \"category\" in term and term[\"category\"] is not None:\n",
    "        return category_concepts[term[\"category\"]] if term[\"category\"] in category_concepts else None\n",
    "    \n",
    "    return thesaurus_concepts[term[\"thesaurus_name\"]] if term[\"thesaurus_name\"] in thesaurus_concepts else None\n",
    "    \n",
    "usgs_thesaurus_terms = list()\n",
    "for term in all_terms:\n",
    "    usgs_thesaurus_terms.append({\n",
    "        \"_date_cached\": str(datetime.datetime.utcnow().isoformat()),\n",
    "        \"source\": term[\"thesaurus_name\"],\n",
    "        \"source_reference\": f\"https://apps.usgs.gov/thesaurus/thesaurus.php?thcode={term['thesaurus_id']}\",\n",
    "        \"url\": add_url(term[\"thesaurus_id\"], term[\"code\"]),\n",
    "        \"label\": term[\"name\"],\n",
    "        \"concept_label\": concept_mapping(term),\n",
    "        \"description\": term[\"scope\"] if term[\"scope\"] is not None and len(term[\"scope\"]) > 20 else None\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata\n",
    "Wikidata provides the most far reaching open knowledge in the world at this point. It is being constantly developed and maintained by librarians and other interested people who key in on one set of concepts or another where the key facts within that area can be encoded and organized in the Wikidata model of entities and claims. We are examining where select pockets of knowledge within the very large (94M+ entities) Wikidata corpus can be used effectively as a reference point.\n",
    "\n",
    "Given the wide open, volunteer contributor nature of Wikidata, there are many things in the system that we cannot fully trust for our purposes. However, Wikidata does often contain the results of highly professional work by librarians and other informaticists who have used the platform to capture public domain data in a way that makes it the most robust source for access to those data. In one particularly notable case, Wikidata provides the most robust and \"machine-actionable\" source for the International Minerological Association's periodic publication on mineral species; a source we have a great deal of use for.\n",
    "\n",
    "We feel we can count on some sources based on the provenance for the entities and claims, the availability and completeness of references and qualifiers on claims, and our ability to examine history of changes to entities to determine any point where we might lose trust in the veracity of the information. We still have more work to do in improving the sophistication on how we select particular information from Wikidata for operational use and will continue to refine the processes encoded into the pyLinkedCMD package itself. In the near term, we are experimenting with the mechanics of gathering information from Wikidata in a most usable form and adding references to our overall reference vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to run the process to build the Wikidata reference vocabulary? [y/N]: \n",
      "CPU times: user 37.6 ms, sys: 19.5 ms, total: 57 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if click.confirm('Are you sure you want to run the process to build the Wikidata reference vocabulary?', default=False):\n",
    "    existing_wd_ref = None\n",
    "    if os.path.exists(isaid_helpers.f_wd_reference):\n",
    "        existing_wd_ref = pickle.load(open(isaid_helpers.f_wd_reference, \"rb\"))\n",
    "    \n",
    "    #wd_reference = wikidata.build_wd_reference(existing_data=existing_wd_ref)\n",
    "    wd_reference = wikidata.build_wd_reference()\n",
    "    pickle.dump(wd_reference, open(isaid_helpers.f_wd_reference, \"wb\"))\n",
    "else:\n",
    "    wd_reference = pickle.load(open(isaid_helpers.f_wd_reference, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA Climate Change Glossary\n",
    "One of the immediate use cases we are exploring for the iSAID capability is related to examining capacity in certain areas of climate change science. To do this, we need to work up a set of terms and concepts that relate specifically to climate change science that can tease our specific topical areas of interest that can be combined together with other characteristics of scientific assets to examine particular questions on where scientific capacity may exist to address new questions and challenges. This will continue to evolve, but the following codeblock explores one possibility through the EPA Climate Change Glossary to add a specific reference point for exploration through named entity recognition processes.\n",
    "\n",
    "It presents an interesting functional case where we want to essentially scrape a textual source whose key concepts have not been formally recognized in another ontology or knowledge system. There are much more comprehensive sources that we are also exploring, namely the Global Change Master Directory Keywords from NASA and aspects of the SWEET ontology pertinent to this domain. However, both of those sources are much more complex and robust and need additional thought, experimentation, and development for reasonable application in training NER processes.\n",
    "\n",
    "In the following codeblock, we read in the original source data from a spreadsheet, filter out some terms that are too broad or could introduce ambiguous results in NER processes, and then add terms in the simplified structure we are using for our reference vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_terms = ['Adaptation',\n",
    " 'Adaptive Capacity',\n",
    " 'Aerosols',\n",
    " 'Annex I Countries/Parties',\n",
    " 'Anthropogenic',\n",
    " 'Atmosphere',\n",
    " 'Biomass',\n",
    " 'Biosphere',\n",
    " 'Borehole',\n",
    " 'Co-Benefit',\n",
    " 'Concentration',\n",
    " 'Earth System',\n",
    " 'Eccentricity',\n",
    " 'Ecosystem',\n",
    " 'Evaporation',\n",
    " 'Feedback Mechanisms',\n",
    " 'Forcing Mechanism',\n",
    " 'Geosphere',\n",
    " 'Hydrologic Cycle',\n",
    " 'Hydrosphere',\n",
    " 'Inundation',\n",
    " 'Landfill',\n",
    " 'Latitude',\n",
    " 'Metric Ton',\n",
    " 'Mitigation',\n",
    " 'Municipal Solid Waste',\n",
    " 'Oxidize',\n",
    " 'Particulate matter',\n",
    " 'Parts Per Billion',\n",
    " 'Parts Per Million by Volume',\n",
    " 'Parts Per Trillion',\n",
    " 'Phenology',\n",
    " 'Photosynthesis',\n",
    " 'Radiation',\n",
    " 'Recycling',\n",
    " 'Reflectivity',\n",
    " 'Residence Time',\n",
    " 'Resilience',\n",
    " 'Respiration',\n",
    " 'Scenarios',\n",
    " 'Sensitivity',\n",
    " 'Short Ton',\n",
    " 'Sink',\n",
    " 'Streamflow',\n",
    " 'Subsiding/Subsidence',\n",
    " 'Teragram',\n",
    " 'Vulnerability',\n",
    " 'Wastewater',\n",
    " 'Water Vapor',\n",
    " 'Weather']\n",
    "\n",
    "df_epa_climate_change_glossary = pd.read_excel(\"https://sor.epa.gov/sor_internet/registry/termreg/searchandretrieve/glossariesandkeywordlists/search.do;jsessionid=87TsZz9rOm0k-CQjrtbfA43w1rPWKUuBf9N25Wg0XwzV9z4-yqOh!1399359231?details=&d-1342820-e=11&6578706f7274=1&glossaryName=Glossary+Climate+Change+Terms&includeInReport=\")\n",
    "epa_climate_change_glossary = list()\n",
    "for index, record in df_epa_climate_change_glossary.iterrows():\n",
    "    if record.Term not in ignore_terms:\n",
    "        term = {\n",
    "            \"_date_cached\": datetime.datetime.utcnow().isoformat(),\n",
    "            \"source\": \"EPA Climate Change Glossary\",\n",
    "            \"source_reference\": \"https://sor.epa.gov/sor_internet/registry/termreg/searchandretrieve/glossariesandkeywordlists/search.do?details=&vocabName=Glossary%20Climate%20Change%20Terms\",\n",
    "            \"label\": record.Term,\n",
    "            \"concept_label\": \"CLIMATE_CHANGE_TERM\",\n",
    "            \"identifier\": f\"epa_climate_change_glossary_{index}\",\n",
    "            \"description\": record.Definitions,\n",
    "            \"label_source\": \"preferred\"\n",
    "        }\n",
    "        epa_climate_change_glossary.append(term)\n",
    "\n",
    "        if isinstance(record.Acronym, str):\n",
    "            alt_term = copy.copy(term)\n",
    "            alt_term[\"label\"] = record.Acronym\n",
    "            alt_term[\"label_source\"] = \"acronym\"\n",
    "            alt_term[\"identifier\"] = f'{term[\"identifier\"]}_alt'\n",
    "            epa_climate_change_glossary.append(alt_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The end result of the processes outlined and executed here is a simple file containing an array of terms that are suitable for entry into our graph as DefinedSubjectMatter entities linked to other entities via some type of relationship (usually an \"ADDRESSES_SUBJECT\" relationship) depending on the origin of the claim. While some of these terms may have started as a property directly part of an entity like a dataset, bringing them into our graph as their own type of entity lets us relate many entities to the same concept and establish a relationship to the reference concept as something more sophisticated than a simple keyword in a list. We are essentially treating these as claims/statements in the Wikidata model where we try to establish at least a date qualifier and a reference on every relationship to one of these terms along with some other type of qualifier, particularly in the case of relationships derived through uncertain NER processes.\n",
    "\n",
    "The following code block brings our separate source materials together into one reference_terms array and shows a summary of the sources and how the source material was classified for NER processing. We then dump it to a file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Wikidata Mineral Species': 10314,\n",
       "         'Wikidata Chemical Elements': 667,\n",
       "         'Wikidata Sedimentary Rocks': 91,\n",
       "         'Wikidata Clastic Sediments': 7,\n",
       "         'Wikidata Sovereign States': 1409,\n",
       "         'Wikidata US States': 50,\n",
       "         'Wikidata Global Seas and Oceans': 258,\n",
       "         'Wikidata Global Faults': 3102,\n",
       "         'Wikidata Global Volcanos': 1548,\n",
       "         'Wikidata Global Earthquakes': 1500,\n",
       "         'Wikidata US National Parks': 106,\n",
       "         'Wikidata US National Monuments': 184,\n",
       "         'Wikidata US National Forests': 221,\n",
       "         'Wikidata US Wild and Scenic Rivers': 50,\n",
       "         'Wikidata Geologic Formations': 9299,\n",
       "         'Wikidata Aquifers': 27,\n",
       "         'Wikidata Fields of Science': 457,\n",
       "         'Wikidata Additional Commodities': 10,\n",
       "         'Wikidata US Territories': 38,\n",
       "         'Wikidata US Counties': 3108,\n",
       "         'EPA Climate Change Glossary': 123,\n",
       "         'Common geographic areas': 66096,\n",
       "         'USGS Thesaurus': 1151,\n",
       "         'Alexandria Digital Library Feature Type Thesaurus': 213,\n",
       "         'Lithologic classification of geologic map units': 207,\n",
       "         'Named instances': 21,\n",
       "         'ISO 19115 Topic Category': 20,\n",
       "         'Data Categories for Marine Planning': 91,\n",
       "         'Thesaurus categories': 30,\n",
       "         'USGS information products': 44,\n",
       "         'Metadata Standards': 11,\n",
       "         'Marine Realms Information Bank (MRIB) keywords': 763,\n",
       "         'Coastal and Marine Ecological Classification Standard': 829})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'MINERAL_SPECIES': 10314,\n",
       "         'CHEMICAL_ELEMENT': 667,\n",
       "         'SEDIMENTARY_ROCK': 91,\n",
       "         'CLASTIC_SEDIMENT': 7,\n",
       "         'SOVEREIGN_STATE': 1409,\n",
       "         'US_STATE': 50,\n",
       "         'SEA_OR_OCEAN': 258,\n",
       "         'GEOLOGIC_FAULT': 3102,\n",
       "         'NAMED_VOLCANO': 1548,\n",
       "         'NAMED_EARTHQUAKE': 1500,\n",
       "         'NATIONAL_PARK': 106,\n",
       "         'NATIONAL_MONUMENT': 184,\n",
       "         'NATIONAL_FOREST': 221,\n",
       "         'WILD_AND_SCENIC_RIVER': 50,\n",
       "         'GEOLOGIC_FORMATION': 9299,\n",
       "         'NAMED_GROUNDWATER_AQUIFER': 27,\n",
       "         'FIELD_OF_SCIENCE': 457,\n",
       "         'GEOLOGIC_COMMODITY_OR_MATERIAL': 10,\n",
       "         'US_TERRITORY': 38,\n",
       "         'US_COUNTY': 3108,\n",
       "         'CLIMATE_CHANGE_TERM': 123,\n",
       "         'USGS_COMMON_GEOGRAPHIC_AREAS': 66096,\n",
       "         'USGS_SCIENCE_TOPICS': 570,\n",
       "         'USGS_SCIENTIFIC_METHODS': 220,\n",
       "         'USGS_SCIENTIFIC_DISCIPLINES': 87,\n",
       "         'USGS_PRODUCT_TYPES': 38,\n",
       "         'USGS_GEOLOGIC_TIME_PERIODS': 39,\n",
       "         'USGS_INSTITUTIONAL_STRUCTURES_ACTIVITIES': 40,\n",
       "         'USGS_BUSINESS_CATEGORIES': 157,\n",
       "         'ALEXANDRIA_DIGITAL_LIBRARY_FEATURE_TYPES': 213,\n",
       "         'USGS_LITHOLOGY': 207,\n",
       "         None: 197,\n",
       "         'ISO_19115_TOPICS))': 20,\n",
       "         'MARINE_REALMS_INFORMATION_BANK': 763,\n",
       "         'CMECS': 829})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reference_terms = wd_reference\n",
    "reference_terms.extend(epa_climate_change_glossary)\n",
    "reference_terms.extend(usgs_thesaurus_terms)\n",
    "display(Counter(i['source'] for i in reference_terms))\n",
    "\n",
    "display(Counter(i['concept_label'] for i in reference_terms if \"concept_label\" in i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(reference_terms, open(isaid_helpers.f_ner_reference, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
