{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works through the process of preparing records from the USGS Publications Warehouse for inclusion in our graph of everything in USGS that we can assemble to build logical catalogs of resources for various purposes. The Pubs Warehouse provides a number of unique, original records that we can't get elsewhere along with additional properties that add value to existing records. In order to work effectively with the entire Pubs Warehouse recordset, we run a regular caching process that houses original source content from a REST API. This provides us with an ability to run aggregations and other queries that help in understanding and working with the catalog metadata.\n",
    "\n",
    "There are over 160K records in the Pubs Warehouse. They are all interesting and useful in differeing circumstances, but we don't necessarily need or want to pull every record into any given graph or index. More records could mean more noise in our queries and analyses depending on what we're trying to accomplish. In the current case, I'm really most interested in developing further intelligence on work that is ongoing now or works that have been authored/edited by staff who are either current or recent. In the following processing steps, I narrow down to pub records for staff who are already in our graph, meaning that they were sourced from the master directory source we got from the ScienceBase Directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isaid_helpers\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import click\n",
    "from copy import copy\n",
    "from pylinkedcmd import utilities\n",
    "import validators\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "from nested_lookup import nested_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pubs Warehouse cache is pretty large if we pull all records. We need to ultimately come up with more ways to take advantage of the cache and run our queries on the server where it lives instead of doing what I'm doing here. In the meantime, the following codeblock can be used to grab the entire cache or load a local stash file into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to run the process to get all Pubs Warehouse data from the cache? [y/N]: \n",
      "pw_cache loaded to memory from cache file\n",
      "CPU times: user 4.09 s, sys: 666 ms, total: 4.75 s\n",
      "Wall time: 8.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if click.confirm('Are you sure you want to run the process to get all Pubs Warehouse data from the cache?', default=False):\n",
    "    pw_cache = isaid_helpers.cache_chs_cache(\"pw\")\n",
    "    pickle.dump(pw_records, open(isaid_helpers.f_process_pw, \"wb\"))\n",
    "else:\n",
    "    pw_cache = pickle.load(open(isaid_helpers.f_process_pw, \"rb\"))\n",
    "    print(\"pw_cache loaded to memory from cache file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function strips down a Pubs Warehouse record into its essential elements, renaming a few properties into the common names we are using in our graph and setting up a couple of more unique properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabularize_pw_record(pw_record):\n",
    "    pub = {\n",
    "        \"name\": pw_record[\"title\"],\n",
    "        \"source\": \"USGS Publications Warehouse\",\n",
    "        \"year_published\": pw_record[\"publicationYear\"],\n",
    "        \"id_pw\": pw_record[\"id\"]\n",
    "    }\n",
    "    if \"docAbstract\" in pw_record and pw_record[\"docAbstract\"] is not None and len(pw_record[\"docAbstract\"]) > 0:\n",
    "        pub[\"description\"] = isaid_helpers.strip_tags(pw_record[\"docAbstract\"])\n",
    "        \n",
    "    if \"ipdsId\" in pw_record and pw_record[\"ipdsId\"] is not None and len(pw_record[\"ipdsId\"]) > 0:\n",
    "        pub[\"id_ipds\"] = pw_record[\"ipdsId\"]\n",
    "    \n",
    "    if \"doi\" in pw_record and pw_record[\"doi\"] is not None and len(pw_record[\"doi\"]) > 0:\n",
    "        pub[\"doi\"] = pw_record[\"doi\"].strip()\n",
    "        \n",
    "    useful_link = None\n",
    "    if \"links\" in pw_record and isinstance(pw_record[\"links\"], list):\n",
    "        useful_link = next((l[\"url\"] for l in pw_record[\"links\"] if \"type\" in l and l[\"type\"][\"text\"] == \"Index Page\"), None)\n",
    "        \n",
    "    if useful_link is None and \"doi\" in pub:\n",
    "        useful_link = f\"https://doi.org/{pub['doi']}\"\n",
    "        \n",
    "    if useful_link is None:\n",
    "        useful_link = f\"https://pubs.er.usgs.gov/publication/{pw_record['indexId']}\"\n",
    "        \n",
    "    pub[\"url\"] = useful_link\n",
    "\n",
    "    if \"contributors\" in pw_record:\n",
    "        if \"authors\" in pw_record[\"contributors\"]:\n",
    "            pub[\"author_emails\"] = \",\".join([i for i in nested_lookup(\"email\", pw_record[\"contributors\"][\"authors\"]) if validators.email(i)])\n",
    "            pub[\"author_orcids\"] = \",\".join([i.split(\"/\")[-1] for i in nested_lookup(\"orcid\", pw_record[\"contributors\"][\"authors\"]) if validators.url(i)])\n",
    "    \n",
    "        if \"editors\" in pw_record[\"contributors\"]:\n",
    "            pub[\"editor_emails\"] = \",\".join([i for i in nested_lookup(\"email\", pw_record[\"contributors\"][\"editors\"]) if validators.email(i)])\n",
    "            pub[\"editor_orcids\"] = \",\".join([i.split(\"/\")[-1] for i in nested_lookup(\"orcid\", pw_record[\"contributors\"][\"editors\"]) if validators.url(i)])\n",
    "\n",
    "    return pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to only pull in publications at this point that can be linked to people in our graph that we've brought in from a master source. In a given workflow, it would be a good idea to update this master information in the graph before querying it for people. Here, we get all person records with email or orcid identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nreinke@contractor.usgs.gov', 'krschulz@usgs.gov', 'alchildress@contractor.usgs.gov', 'mkkelley@usgs.gov', 'astanley@usgs.gov']\n",
      "['0000-0002-5275-3077', '0000-0003-2409-5211', '0000-0003-1155-2815', '0000-0003-4147-7254', '0000-0001-5756-0373']\n"
     ]
    }
   ],
   "source": [
    "with isaid_helpers.graph_driver.session(database=isaid_helpers.graphdb) as session:\n",
    "    results = session.run(\"\"\"\n",
    "    MATCH (p:Person)\n",
    "    WHERE NOT p.email IS NULL\n",
    "    OR NOT p.orcid IS NULL\n",
    "    RETURN p.email, p.orcid, p.name\n",
    "    \"\"\")\n",
    "    linkable_persons = results.data()\n",
    "\n",
    "emails_in_graph = [i[\"p.email\"] for i in linkable_persons if i[\"p.email\"] is not None]\n",
    "orcids_in_graph = [i[\"p.orcid\"] for i in linkable_persons if i[\"p.orcid\"] is not None]\n",
    "\n",
    "print(emails_in_graph[:5])\n",
    "print(orcids_in_graph[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the emails and orcids in our graph and our full Pubs Warehouse cache in memory, we can figure out which pubs we should pull into a subset for sending to our graph. We loop through our cached pub records, check for emails and orcids in the record using a elegant little package (nested_lookup) and process anything where at least one of those identifiers is in our graph. The tabularize_pw_record function simplifies and flattens a basic record for the pub. We go ahead and dump this table to a CSV file for processing into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24304\n",
      "CPU times: user 14.7 s, sys: 159 ms, total: 14.8 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graphable_pw = list()\n",
    "\n",
    "for pub in pw_cache:\n",
    "    emails_in_pub = [i for i in nested_lookup(\"email\", pub) if validators.email(i)]\n",
    "    orcids_in_pub = [i.split(\"/\")[-1] for i in nested_lookup(\"orcid\", pub) if validators.url(i)]\n",
    "    if next((e for e in emails_in_pub if e in emails_in_graph), None) is not None or next((o for o in orcids_in_pub if o in orcids_in_graph), None) is not None:\n",
    "        graphable_pw.append(tabularize_pw_record(pub))\n",
    "\n",
    "pd.DataFrame(graphable_pw).to_csv(isaid_helpers.f_graphable_pw, index=False)\n",
    "print(len(graphable_pw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
